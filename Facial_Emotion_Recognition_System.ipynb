{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facial Emotion Recognition System.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/primaa28/Facial-Emotion-Recognition-System./blob/master/Facial_Emotion_Recognition_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TVkmrBfQD5o"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDe7UcAdQra4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "372f15fc-e3e2-4747-e6b9-ebcbb1dc3ca0"
      },
      "source": [
        "\n",
        "df = pd.read_csv(r'/content/fer2013.csv')\n",
        "\n",
        "\n",
        "\n",
        "df.head(35887)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24742</th>\n",
              "      <td>6</td>\n",
              "      <td>0 0 4 4 2 0 0 0 2 3 2 6 10 5 3 6 5 13 8 9 51 5...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24743</th>\n",
              "      <td>3</td>\n",
              "      <td>8 28 41 55 61 59 55 82 84 80 88 135 117 105 12...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24744</th>\n",
              "      <td>4</td>\n",
              "      <td>255 255 255 255 255 255 255 255 255 255 255 25...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24745</th>\n",
              "      <td>6</td>\n",
              "      <td>75 76 80 82 88 93 102 112 124 130 137 144 155 ...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24746</th>\n",
              "      <td>3</td>\n",
              "      <td>255 255 255 255 255 255 255 255 255 255 255 25...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24747 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       emotion                                             pixels     Usage\n",
              "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1            0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2            2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
              "...        ...                                                ...       ...\n",
              "24742        6  0 0 4 4 2 0 0 0 2 3 2 6 10 5 3 6 5 13 8 9 51 5...  Training\n",
              "24743        3  8 28 41 55 61 59 55 82 84 80 88 135 117 105 12...  Training\n",
              "24744        4  255 255 255 255 255 255 255 255 255 255 255 25...  Training\n",
              "24745        6  75 76 80 82 88 93 102 112 124 130 137 144 155 ...  Training\n",
              "24746        3  255 255 255 255 255 255 255 255 255 255 255 25...       NaN\n",
              "\n",
              "[24747 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2_7YsIGkf3v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adc8a852-cdf1-42de-d020-9c00f1def80e"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(249, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCEbmntVlG-K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "45cae589-85b6-4eb2-c1d3-bbd31dcbfd98"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 43386 entries, 0 to 43385\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  43386 non-null  object\n",
            " 1   pixels   43386 non-null  object\n",
            " 2   Usage    43384 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 1017.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFwv40-hRxtc"
      },
      "source": [
        "#converting the string values of pixels into array\n",
        "def arr_conv (strg): \n",
        "    b=[]\n",
        "    a=strg.split(\" \")\n",
        "    for elements in a:\n",
        "        b.append(int (elements))\n",
        "    \n",
        "    return np.array(b)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU50aqYdSdK5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "8d4e7824-405f-4efd-9ce4-77969e68c135"
      },
      "source": [
        "df[\"pixels_arr\"]= df['pixels'].apply(arr_conv)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cdaf8647d7d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixels_arr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-1fc3c74d335a>\u001b[0m in \u001b[0;36marr_conv\u001b[0;34m(strg)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melements\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WNDb_NvSf1x"
      },
      "source": [
        "min(df[\"pixels_arr\"][100])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__j4noVvVhza"
      },
      "source": [
        "max(df[\"pixels_arr\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig-80ZfzWIEv"
      },
      "source": [
        "image,label = df[\"pixels_arr\"][0],df[\"emotion\"][0]\n",
        "print('Shape:', image.shape, '\\nLabel:', label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggo5iW8I_-HR"
      },
      "source": [
        "plt.imshow(df[\"pixels_arr\"][106].reshape(48,48),cmap='gray');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kE8-_AaADom"
      },
      "source": [
        "len(df['pixels_arr'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh8CEM-kAKZ9"
      },
      "source": [
        "type(df['pixels_arr'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kSDRzq_AKko"
      },
      "source": [
        "train_data_fer= torch.Tensor(df['pixels_arr']) ##important step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX2KkWHAAKsz"
      },
      "source": [
        "train_data_fer[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njNTiQHfgw1s"
      },
      "source": [
        "train_data_fer.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT4ef51RgxAz"
      },
      "source": [
        "train_data_fer= train_data_fer.reshape(35887,1,48,48) #reshaping to 1 color channel and 48X48 image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZPh9fvwgxK6"
      },
      "source": [
        "train_data_fer.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lF4rfWDg-yX"
      },
      "source": [
        "train_data_fer[0].shape # 1st image as a tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9Aexat3g_Bb"
      },
      "source": [
        "train_data_fer[0:5].shape #first 5 images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7InIMoP2g_Fu"
      },
      "source": [
        "plt.imshow(train_data_fer[0].reshape(48,48),cmap='gray');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66uY4Zmig_JB"
      },
      "source": [
        "class_names= ['Angry', 'Disgust','Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0asuTJ3g_O8"
      },
      "source": [
        "labels= list(df['emotion'][0:5])\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMwkUWnGg_Mz"
      },
      "source": [
        "np.set_printoptions(formatter=dict(int=lambda x: f'{x:5}')) # to widen the printed array\n",
        "from torchvision.utils import make_grid\n",
        "labels =list(df['emotion'][0:5])\n",
        "print('Label:', np.array(labels))\n",
        "print('Class: ', *np.array([class_names[i] for i in labels]))\n",
        "images= train_data_fer[0:5]\n",
        "im = make_grid(images, nrow=5)\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.imshow(np.transpose(im, (1, 2, 0)));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMp_K4aVhcsb"
      },
      "source": [
        "Y=torch.Tensor(df['emotion'])# target variable values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orwsqNutiUQU"
      },
      "source": [
        "Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQHyiovzhcz5"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV1OUN_liYHM"
      },
      "source": [
        "Y = Y.type(torch.LongTensor)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTUUKjUKiYOP"
      },
      "source": [
        "Y.type()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XF1iiC0ie6A"
      },
      "source": [
        "Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6YN2656ie-C"
      },
      "source": [
        "len(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq1jCr13ifBL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test=train_test_split(train_data_fer, Y,test_size=0.20, random_state=23)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqjOLMUdiYSP"
      },
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aMLgUMTj0q3"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWyFSKfWjeIW"
      },
      "source": [
        "y_train.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0qyKInjjeZd"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtT-sxPJjemd"
      },
      "source": [
        "train_dataset = TensorDataset(x_train, y_train)  #imp step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tGUIihtj8Ix"
      },
      "source": [
        "test_dataset = TensorDataset(x_test, y_test)  #imp step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN7tMPn8j8ME"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj7ulfz7j8dj"
      },
      "source": [
        "train_dataset[0][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6f-ayv0kGpU"
      },
      "source": [
        "test_dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkR2YuQ8kG-R"
      },
      "source": [
        "test_dataset[0][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ykA1wbbkHJe"
      },
      "source": [
        "type(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJi7getskW07"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE7jeTqQkXG4"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\"\"\"\n",
        "Initializing train/test dataloader object which splits the training dataset into small batches of 50 images each\n",
        "and this can be used later in the CNN model  \n",
        "\"\"\"\n",
        "torch.manual_seed(101)\n",
        "bat_sz=50\n",
        "train_loader = DataLoader(train_dataset,batch_size=bat_sz,shuffle=True)\n",
        "test_loader = DataLoader(test_dataset,batch_size=bat_sz,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e2xOvGakXQY"
      },
      "source": [
        "len(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69b6-JB1kXWv"
      },
      "source": [
        "len(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ3_ebEskpK6"
      },
      "source": [
        "Then we have 7 emotions that we are predicting namely (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral), so we have 7 labels. We will be processing our inputs with a batch size of 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IompENfkkVu"
      },
      "source": [
        "class CONVNN(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.conv1  = nn.Conv2d(1, 300 , kernel_size =5, stride=1,padding=2)#1 color channels as input and We will be using 5X5 filter/kernel and stride of 1 without any padding \n",
        "        self.norm1  = nn.BatchNorm2d(300)\n",
        "        #self.drop1  = nn.Dropout2d(p=0.2)\n",
        "        self.conv2  = nn.Conv2d(300, 150, 5 , 1,padding=2)\n",
        "        self.norm2  = nn.BatchNorm2d(150)\n",
        "        self.conv3  = nn.Conv2d(150, 75, 5 , 1,padding=2)\n",
        "        self.norm3  = nn.BatchNorm2d(75)\n",
        "        #self.drop2  = nn.Dropout2d(p=0.2)\n",
        "        self.layer1 = nn.Linear(6*6*75,180) # we need to calculate the resulting number of matrices passing thru the conv layer and put that corressponding o/p as the linear layer input\n",
        "        self.drop3  = nn.Dropout2d(p=0.1)\n",
        "        self.layer2 = nn.Linear(180,84)\n",
        "        self.layer3 = nn.Linear(84,7) #only 7 classes of Dogs and Cats\n",
        "        \n",
        "    def forward(self,x):\n",
        "                         #self.drop1\n",
        "        x= F.max_pool2d(             ( F.relu(    self.norm1(self.conv1(x)) )  )      ,2,2) #adding relu and max pooling with 2x2 kernel and stride of 2\n",
        "        \n",
        "        x= F.max_pool2d(             ( F.relu(    self.norm2(self.conv2(x)) )  )      ,2,2) #adding relu and max pooling the same line\n",
        "        x= F.max_pool2d(             ( F.relu(    self.norm3(self.conv3(x)) )  )      ,2,2) #adding relu and max pooling with 2x2 kernel and stride of 2\n",
        "\n",
        "        x= self.drop3(F.relu(      self.layer1(  x.view(-1,6*6*75)   ) ) )      #flattening by using View and no dropout has been added\n",
        "        x= F.relu(self.layer2(x))\n",
        "        x= F.log_softmax(  self.layer3(x), dim=1)          #multi class classificaiton\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf_IkPHakkbS"
      },
      "source": [
        "torch.manual_seed(101)\n",
        "model = CONVNN()#to instansiate the model as cuda use \"model = CONVNN().to(device) \" or \"model.cuda()\" so that whatever the tensors we are passing to the model will be saved in GPU and operations will be performed on that\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omr1jrinkkic"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh12ik4Akkfh"
      },
      "source": [
        "epochs = 8\n",
        "train_loss= []\n",
        "test_loss= []\n",
        "train_acc=[]\n",
        "test_acc = []\n",
        "\n",
        "print(f'\\nConvolutional Neural Network Model Metrics:\\n')\n",
        "print(f'\\t This CNN model configuration has {epochs} epochs with each batch size of {bat_sz} images:\\n')\n",
        "for i in range(epochs):\n",
        "    \n",
        "    train_crt_pred = 0\n",
        "    test_crt_pred = 0\n",
        "    conf_mat= torch.FloatTensor([])\n",
        "    \n",
        "    for b,(x_train,y_train) in enumerate (train_loader):\n",
        "        b += 1\n",
        "        y_pred = model.forward(x_train)\n",
        "        loss= criterion(y_pred,y_train.long() )\n",
        "        \n",
        "        buffer = torch.max(y_pred.data, 1) [1]\n",
        "        batch_acc = (buffer == y_train).sum()\n",
        "        train_crt_pred +=  batch_acc\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "                \n",
        "        if b%  int((len(train_dataset)/bat_sz)/3 )  == 0:\n",
        "            print(f'Epoch{i+1:2} Batch {b:4} loss: {loss.item():5.2f} Train Accuracy: {train_crt_pred.item()*100/(bat_sz*b):6.3f}%')\n",
        "    \n",
        "    train_loss.append(loss)  #loss after 1 epoch\n",
        "    train_acc.append(train_crt_pred) # crt predictions after 1 epoch\n",
        "    \n",
        "    \n",
        "    with torch.no_grad():  #testing after 1 complete epoch\n",
        "        for b,(x_test,y_test) in enumerate (test_loader):\n",
        "            b += 1\n",
        "            y_eval = model(x_test)\n",
        "            loss= criterion(y_eval,y_test.long())\n",
        "            buffer1 = torch.max(y_eval.data, 1) [1]\n",
        "            conf_mat =  torch.cat((conf_mat.float(),buffer1.float()),0)\n",
        "            batch_acc = (buffer1 == y_test).sum()\n",
        "            test_crt_pred +=  batch_acc\n",
        "            test_loss.append(loss) #test loss after the last completed epoch\n",
        "    test_acc.append(test_crt_pred) # crt predictions using the last completed epoch\n",
        "\n",
        "    print(f'After {i+1} Epoch(s) the Train Accuracy is {(train_crt_pred.item()/len(train_dataset))*100:6.3f}% and Test Accuracy is {(test_crt_pred.item()/len(test_dataset))*100:6.3f}% \\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94ohFaTjU-3X"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import seaborn as sns\n",
        "print('\\nThe Confusion Matrix is plotted below:')\n",
        "cfmt =pd.DataFrame(confusion_matrix(torch.Tensor([r for q,r in test_dataset]).reshape(-1,1),conf_mat.reshape(-1,1),labels=[0,1,2,3,4,5,6]),index=class_names,columns=class_names)\n",
        "plt.figure(figsize=(12,9))\n",
        "sns.heatmap(cfmt,annot=True,cmap='BuGn',fmt=\"d\")\n",
        "plt.xlabel(\"Prediction\",fontsize=14)\n",
        "plt.ylabel(\"Labels\",fontsize=14)\n",
        "plt.show()\n",
        "print('\\nThe Classification Report is plotted below: \\n\\n',classification_report(torch.Tensor([r for q,r in test_dataset]).reshape(-1,1),conf_mat.reshape(-1,1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABh6625jVNyx"
      },
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "plt.plot(train_loss, label='training loss')\n",
        "plt.plot(test_loss, label='validation loss')\n",
        "plt.title('Loss at the end of each epoch')\n",
        "plt.legend();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIvhTZL9VUyc"
      },
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "plt.plot([(t/(len(train_dataset)/100)) for t in train_acc], label='Train score',marker='o',alpha=0.8)\n",
        "plt.plot([(t/(len(test_dataset)/100)) for t in test_acc], label='Test score',marker='o')\n",
        "plt.title('\\nTrain Vs Test Accuracy over Epochs\\n',fontsize=16)\n",
        "# plt.xticks(np.arange(0,20,1));\n",
        "plt.xlabel('Epochs',fontsize=14)\n",
        "plt.ylabel('Accuracy Metrics',fontsize=14)\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1O9j2XeVXXq"
      },
      "source": [
        "# Evaluation DATA TRANSFORMATION\n",
        "eval_transform =transforms.Compose( [\n",
        "    transforms.Resize( (48,48)),\n",
        "    transforms.Grayscale(num_output_channels=1),# Third transformation --Resize to 224 as the mean in the describe function above\n",
        "    transforms.ToTensor(),                     # Fifth transformation -- to convert it into tensor array (make sure the tensor converion is at the last step)\n",
        "    \n",
        "] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kD-Ut91ubIi"
      },
      "source": [
        "root = r'C:\\Univ'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcn9vTLOy1bB"
      },
      "source": [
        "import os\n",
        "from PIL import Image #to operate on pics or images\n",
        "from IPython.display import display #Only for Jupyter Notebook\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "eval_data  = datasets.ImageFolder(os.path.join(root,'expr'),transform = eval_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwYNUmxzzyGK"
      },
      "source": [
        "eval_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pJfZOLZz1A-"
      },
      "source": [
        "image, label = eval_data[0]\n",
        "print('Shape:', image.shape, '\\nLabel:', label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XIVFEcC0DK1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axjw_oLA0DXc"
      },
      "source": [
        "np.set_printoptions(formatter=dict(int=lambda x: f'{x:5}')) # to widen the printed array\n",
        "from torchvision.utils import make_grid\n",
        "# Grab the first  10 images from the first batch of training data\n",
        "eval_load_all = DataLoader(eval_data, batch_size=2, shuffle=True)\n",
        "for images,labels in eval_load_all: \n",
        "    break\n",
        "\n",
        "# Print the labels\n",
        "print('Label:', labels.numpy())\n",
        "print('Class: ', *np.array([class_names[i] for i in labels]))\n",
        "\n",
        "# Print the images\n",
        "im = make_grid(images, nrow=2)  # the default nrow is 8\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptSYi6BQ0YWk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "5f8a6a95-018c-49ce-e525-e047ee56dc29"
      },
      "source": [
        "eval_load_all = DataLoader(eval_data, batch_size=2, shuffle=False)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    for X_test, y_test in eval_load_all:\n",
        "        y_val = model(X_test)\n",
        "        predicted = torch.max(y_val,1)[1]\n",
        "        correct += (predicted == y_test).sum()\n",
        "        \n",
        "print(f'Test accuracy: {correct.item()}/{len(eval_data)} = {correct.item()*100/(len(eval_data)):7.3f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7bb550bf8ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_load_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_load_all\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
          ]
        }
      ]
    }
  ]
}